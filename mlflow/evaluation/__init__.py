from typing import Any, Dict, List, Optional, Union

import pandas as pd

from mlflow.entities import Evaluation, Feedback, Metric


def log_evaluation(
    *,
    run_id: str,
    inputs: Dict[str, Any],
    outputs: Dict[str, Any],
    inputs_id: Optional[str] = None,
    request_id: Optional[str] = None,
    ground_truths: Optional[Dict[str, Any]] = None,
    feedback: Optional[Union[List[Feedback], List[Dict[str, Any]]]] = None,
    metrics: Optional[Union[List[Metric], Dict[str, float]]] = None,
):
    """
    Logs an evaluation to an MLflow Run.

    Args:
      run_id (str): ID of the MLflow Run to log the Evaluation.
      inputs (Dict[str, Any]): Input fields used by the model to compute outputs.
      outputs (Dict[str, Any]): Outputs computed by the model.
      inputs_id (Optional[str]): Unique identifier for the evaluation `inputs`. If not specified,
          a unique identifier is generated by hashing the inputs.
      request_id (Optional[str]): ID of an MLflow Trace corresponding to the inputs and outputs.
          If specified, displayed in the MLflow UI to help with root causing issues and identifying
          more granular areas for improvement when reviewing the evaluation and adding feedback.
      ground_truths (Optional[Dict[str, Any]]): Ground truths corresponding to one or more of the
          evaluation `outputs`. Helps root cause issues when reviewing the evaluation and adding
          feedback.
      feedback (Optional[Union[List[Feedback], List[Dict[str, Any]]]]): Feedback on the evaluation,
          e.g., relevance of documents retrieved by a RAG model to a user input query, as assessed
          by an LLM Judge.
      metrics (Optional[Union[List[Metric], Dict[str, float]]]): Numerical metrics for the
          evaluation, e.g., "number of input tokens", "number of output tokens".
    """
    # Implementation for logging an evaluation


def log_evaluations(*, run_id: str, evaluations: List[Evaluation]):
    """
    Logs one or more evaluations to an MLflow Run.

    Args:
      run_id (str): ID of the MLflow Run to log the Evaluations.
      evaluations (List[Evaluation]): List of one or more MLflow Evaluation objects.
    """
    # Implementation for logging multiple evaluations


def log_evaluations_df(
    *,
    run_id: str,
    evaluations_df: pd.DataFrame,
    input_cols: List[str],
    output_cols: List[str],
    ground_truth_cols: Optional[List[str]] = None,
    feedback_cols: Optional[List[str]] = None,
) -> pd.DataFrame:
    """
    Logs one or more evaluations from a DataFrame to an MLflow Run.

    Args:
      run_id (str): ID of the MLflow Run to log the Evaluations.
      evaluations_df (pd.DataFrame): Pandas DataFrame containing the evaluations to log.
          Must contain the columns specified in `input_cols`, `output_cols`, and
          `ground_truth_cols`.
          Additionally, evaluation information will be read from the following optional columns,
          if specified (see documentation for the log_evaluations() API):
              - "inputs_id": Unique identifier for evaluation inputs.
              - "request_id": ID of an MLflow trace corresponding to the evaluation inputs and
                  outputs.
              - "metrics": Numerical evaluation metrics, represented as a list of MLflow Metric
                  objects or as a dictionary.
      input_cols (List[str]): Names of columns containing input fields for evaluation.
      output_cols (List[str]): Names of columns containing output fields for evaluation.
      ground_truth_cols (Optional[List[str]]): Names of columns containing ground truths for
          evaluation.
      feedback_cols (Optional[List[str]]): Names of columns containing feedback for evaluation.
          Values must be MLflow Feedback objects or dictionary representations of MLflow Feedback
          objects.

    Returns:
      pd.DataFrame: The specified evaluations DataFrame, with an additional "evaluation_id" column
          containing the IDs of the logged evaluations.
    """
    # Implementation for logging evaluations from a DataFrame
